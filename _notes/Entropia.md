# Entropia de Shannon

- Segundo a teoria da informação, desenvolvida por Claude Shannon, é a medida de **incerteza**, logo, da quantidade média de informação necessária/contida em uma fonte de dados.

- A entropia é **expressa em bits**, e é usada para quantificar a *"surpresa"* associada a uma mensagem ou evento específico.
	- Uma entropia de n bits significa que, em média, n bits são necessários para codificar cada símbolo da fonte.

- Quanto maior a entropia, maior a incerteza ou "supresa", logo maior o número de bits necessário para representar tais possibilidades.

1. **Probabilidade e Incerteza** - Quanto maior a probabilidade e um evento, menor é a contribuição desse evento para a entropia. Isso porque, eventos mais prováveis, carregam **menos** informações "surepreendentes". Eventos menos prováveis, contribuem **mais** para a entropia. A relação é inversamente proporcional.

<img src="https://i.ytimg.com/vi/0QA7eemva0k/maxresdefault.jpg">


2. **Entropia Máxima** - Quando todos os eventos são **igualmente prováveis**, a entropia é máxima! Nesse caso, a **incerteza** é MAIOR, e a entropia é igual ao logaritmo do número de eventos possíveis.

